{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_HW3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zehavitc/NLP/blob/master/NLP_HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQIsj6P9tf_O",
        "colab_type": "text"
      },
      "source": [
        "**HW3**\n",
        "\n",
        "Matan Sabag 301388567\n",
        "\n",
        "Zehavit Leibovich 305226391"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD_0Qdm7tV3O",
        "colab_type": "text"
      },
      "source": [
        "Part 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvOtx_rcnRQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0JTWnYVn0RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load(model_name):\n",
        "  '''\n",
        "  Downloading and loading model into memory, as a dictionary of arrays, the keys are the words.\n",
        "  '''\n",
        "  wv_from_bin = api.load(model_name)\n",
        "  vocab = list(wv_from_bin.vocab.keys())\n",
        "  print(\"Loaded vocab size %i\" % len(vocab))\n",
        "  return wv_from_bin\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWN9aZvmtb_0",
        "colab_type": "text"
      },
      "source": [
        "Part 2 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAMpGfJaoTm8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d90d9575-10bb-4931-c506-170697404a6d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGToqX4Kp9sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "analogy_file_path = \"/content/drive/My Drive/NLP/HW3/analogy.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wn5z0w_xSlK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "797ad306-762d-4461-ddc1-c05cf0603bba"
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "\n",
        "groups_names = [\"capital-world\", \"currency\", \"city-in-state\", \"family\", \"gram1-adjective-to-adverb\", \"gram2-opposite\", \"gram3-comparative\", \"gram6-nationality-adjective\"]\n",
        "models_names = [\"glove-wiki-gigaword-300\",\"glove-wiki-gigaword-50\",\"word2vec-google-news-300\", \"glove-twitter-200\", \"fasttext-wiki-news-subwords-300\"]\n",
        "\n",
        "results = pd.DataFrame(columns = groups_names + [\"Overall\"])\n",
        "for model_name in models_names:\n",
        "  model = load(model_name)\n",
        "  analogy_score, sections = model.wv.evaluate_word_analogies(datapath(analogy_file_path))\n",
        "  results.loc[model_name, \"Overall\"] = analogy_score\n",
        "  for section in sections:\n",
        "    group_name = section['section']\n",
        "    if group_name in groups_names:\n",
        "      results.loc[model_name, group_name] =  len(section['correct']) / (len(section['correct']) + len(section['incorrect']))\n",
        "  del model\n",
        "  gc.collect()\n",
        "  \n",
        "results\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded vocab size 400000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded vocab size 400000\n",
            "Loaded vocab size 3000000\n",
            "Loaded vocab size 1193514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gclaoEGi6T3c",
        "colab_type": "text"
      },
      "source": [
        "Part 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVXCPIq16Yjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_embeddings(data, labels):\n",
        "    plt.scatter(data[:, 0], data[:, 1], marker='o')\n",
        "\n",
        "    for label, x, y in zip(labels, data[:, 0], data[:, 1]):\n",
        "        plt.annotate(\n",
        "            label,\n",
        "            xy=(x, y), xytext=(-20, 20),\n",
        "            textcoords='offset points', ha='right', va='bottom',\n",
        "            bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.2),\n",
        "            arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
        "\n",
        "# Testing        \n",
        "show_embeddings(np.array([[1, 2],[2, 3]]), [\"a\",\"b\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtMGWXO_6aui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def reduce_dim(model, number_of_vectors=10000, required_words=[\"car\"]):\n",
        "  '''\n",
        "  number_of_vectors: you should first truncate your model to only this number of vectors. Make sure to shuffle the model first, so that you choose random vectors.\n",
        "  required_words: in addition to the number_of_vectors vectors you keep, you should keep the vectors of these words.\n",
        "  Return the new, reduced model.\n",
        "  '''\n",
        "  words = list(model.vocab.keys())\n",
        "  np.random.shuffle(words)\n",
        "  X = []\n",
        "  for word in (words[:number_of_vectors] + required_words):\n",
        "    X.append(model.get_vector(word))\n",
        "    \n",
        "  X = np.asarray(X)\n",
        "  reducer  = TruncatedSVD(n_components = 2)\n",
        "  reducer.fit(X)\n",
        "  return reducer.transform(X)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEJhlmjP6yD9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "required_words = [\"aircraft\", \"ambulance\", \"bicycle\", \"bike\", \"boat\", \"car\", \"truck\", \"chopper\", \"canoe\", \"driver\", \"lorry\", \"suv\", \"train\",\n",
        "                  \"vehicle\", \"yacht\", \"apple\", \"banana\", \"orange\", \"tomato\", \"pepper\", \"watermelon\", \"grapes\", \"fruit\", \"fruits\", \"vegetables\"]\n",
        "model = load(\"glove-wiki-gigaword-50\")\n",
        "reduced_model = reduce_dim(model, number_of_vectors=10000, required_words=required_words)\n",
        "show_embeddings(reduced_model[10000:], required_words)\n",
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNFO2ViqCUhq",
        "colab_type": "text"
      },
      "source": [
        "Part 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFE7uRiSCWiD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "brown_w2v = Word2Vec(brown.sents())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95TiKjp1cuRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_term_sentences(corpus, term):\n",
        "  res = []\n",
        "  for sentence in corpus.sents():\n",
        "    str_sent = ' '.join(sentence)\n",
        "    lowered_sent = str_sent.lower() \n",
        "    if term in lowered_sent:\n",
        "      res.append(str_sent)\n",
        "  return res\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDlm78V9cwHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_vector(model_w2v, word):  \n",
        "  if word in model_w2v:\n",
        "    return model_w2v[word]\n",
        "  \n",
        "  if word.lower() in model_w2v:\n",
        "    return model_w2v[word.lower()]\n",
        "  \n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCZmFR12cyCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_neighbour_words(sentences, term, take_before):\n",
        "  res = []\n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    term_idx = [i for i, word in enumerate(words) if word == term]\n",
        "    for idx in term_idx:\n",
        "      if take_before:\n",
        "        if idx > 0:\n",
        "          res.append(words[idx - 1])\n",
        "        if idx > 1:\n",
        "          res.append(words[idx - 2])\n",
        "      elif not(take_before):\n",
        "        if idx < len(words) - 1:\n",
        "          res.append(words[idx+1])\n",
        "        if idx < len(words) - 2:\n",
        "          res.append(words[idx + 2])\n",
        "  return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRicC9RNcz83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "terms = [\"new york\", \"united states\"]\n",
        "for term in terms:\n",
        "  term_words = term.split()\n",
        "  sentences = get_term_sentences(brown, term)\n",
        "  neighbours = get_neighbour_words(sentences, term_words[0], True) + get_neighbour_words(sentences, term_words[1], False)\n",
        "  neighbours_vectors = []\n",
        "  for word in neighbours:\n",
        "    word_vec = get_word_vector(brown_w2v, word)\n",
        "    if word_vec is not None:\n",
        "      neighbours_vectors.append(word_vec)\n",
        "    \n",
        "  term_vec = np.array(neighbours_vectors)\n",
        "  print(f'{term} vector: {np.mean(term_vec, axis=0)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd3nvyJJc2lD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}