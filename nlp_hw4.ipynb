{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_hw4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zehavitc/NLP/blob/master/nlp_hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13rcXIzhXWUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import deque"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RMqfQgIDxxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conll_to_transitions(sentence):\n",
        "    '''\n",
        "    Given a sentence, returns a list of transitions.\n",
        "    Each transition is a training instance for your classifier. A transition \n",
        "    is composed of the following 4 items:\n",
        "    - first word in stack\n",
        "    - second word in stack (could be None is stack is of size=1)\n",
        "    - first word in buffer (could be None if the buffer is empty)\n",
        "    - the transition label (SHIFT, LEFT, RIGHT)\n",
        "    '''\n",
        "    s = []  #stack\n",
        "    b = deque([])  #buffer\n",
        "\n",
        "    transitions = []\n",
        "\n",
        "    for w in sentence:\n",
        "        b.append(w)\n",
        "\n",
        "    s.append(['0', 'ROOT', '_', '_', '_', '_', '0', '_', '_', '_'])\n",
        "\n",
        "    while len(b) > 0 or len(s) > 1:\n",
        "        if s[-1][0] == '0':   # the root\n",
        "            add_shift(s, b, transitions)\n",
        "        elif s[-2][6] == s[-1][0] and check_rightest_arc(s[-2], b):\n",
        "            add_left(s, b, transitions)\n",
        "        elif s[-1][6] == s[-2][0] and (len(b) == 0 or s[-2][0] != '0') and check_rightest_arc(s[-1], b):\n",
        "            add_right(s, b, transitions)\n",
        "        elif len(b) == 0:\n",
        "            #print(\"Non projective\")\n",
        "            return None\n",
        "        else:\n",
        "            add_shift(s, b, transitions)\n",
        "    return transitions\n",
        "\n",
        "\n",
        "def check_rightest_arc(word, b):\n",
        "    '''\n",
        "   w[6] is the index of the head of \"this\" word, so in this method we check\n",
        "   if there is an arc that goes from one of the words in the buffer\n",
        "   to \"word\" (which exists in the stack)\n",
        "    '''\n",
        "    for w in b:\n",
        "        if w[6] == word[0]:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def add_shift(s, b, transitions):\n",
        "    '''\n",
        "    Adding shift transition\n",
        "    '''\n",
        "    word = b.popleft()\n",
        "    top2 = None\n",
        "    if len(s) > 1:\n",
        "        top2 = s[-2]\n",
        "    transitions.append([s[-1], top2, word, 'SHIFT'])\n",
        "    s.append(word)\n",
        "\n",
        "\n",
        "def add_left(s, b, transitions):\n",
        "    '''\n",
        "    Adding left transition\n",
        "    '''\n",
        "    top1 = s.pop()\n",
        "    top2 = s.pop()\n",
        "    transitions.append([top1, top2, b[0] if len(b) > 0 else None, 'LEFT'])\n",
        "    s.append(top1)\n",
        "\n",
        "\n",
        "def add_right(s, b, transitions):\n",
        "    '''\n",
        "    Adding right transition\n",
        "    '''\n",
        "    top1 = s.pop()\n",
        "    top2 = s.pop()\n",
        "    transitions.append([top1, top2, b[0] if len(b) > 0 else None, 'RIGHT'])\n",
        "    s.append(top2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlg_o8EgQ8hH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfT4yExZQ_IJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz3p8U8IREgb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "3fb7b47c-abc9-43f9-feb7-b031a3088f9d"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GGP7xpcWRbf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = \"/content/drive/My Drive/NLP/HW4/train\"\n",
        "eval_path = \"/content/drive/My Drive/NLP/HW4/eval\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trTprxiPUrpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter, defaultdict\n",
        "\n",
        "\n",
        "word_idx_idx = 0\n",
        "word_idx = 1\n",
        "pos_tag_idx = 3\n",
        "ps_tag_specific_idx = 4\n",
        "head_idx = 6\n",
        "dep_label_idx = 7\n",
        "\n",
        "\n",
        "def get_sentences(file_path):\n",
        "  sentences = []\n",
        "  sentence = []\n",
        "  word_current_idx = 0\n",
        "  pos_current_idx = 0\n",
        "  word_encoding = {}\n",
        "  pos_encoding = {}\n",
        "  words_pos = {}\n",
        "  \n",
        "  with open(file_path) as file:  \n",
        "    for line in file:\n",
        "      if line.strip() == '':\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "        continue\n",
        "      # line is part of a sentence - parse the line \n",
        "      splitted_line = line.split()\n",
        "      lower_word = splitted_line[word_idx].lower()\n",
        "      if lower_word not in word_encoding:\n",
        "        word_encoding[lower_word] = word_current_idx\n",
        "        word_current_idx += 1 \n",
        "      pos_tag = splitted_line[pos_tag_idx]\n",
        "      if lower_word not in words_pos:\n",
        "        words_pos[lower_word] = pos_tag\n",
        "      if pos_tag not in pos_encoding:\n",
        "        pos_encoding[pos_tag] = pos_current_idx\n",
        "        pos_current_idx += 1\n",
        "      sentence.append(splitted_line)\n",
        "      \n",
        "  return sentences, word_encoding, pos_encoding, words_pos\n",
        "      \n",
        "      \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEerimsZhjFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_one_hot(encoding, key):\n",
        "    res = np.zeros(len(encoding))\n",
        "    if (key == \"root\" or key is None):\n",
        "      return res\n",
        "    res[encoding[key]] = 1\n",
        "    return res\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcovcD_KM2Xe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "96e2d230-cc29-40f9-e974-1ead407aa6d0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "def load(model_name):\n",
        "  '''\n",
        "  Downloading and loading model into memory, as a dictionary of arrays, the keys are the words.\n",
        "  '''\n",
        "  wv_from_bin = api.load(model_name)\n",
        "  vocab = list(wv_from_bin.vocab.keys())\n",
        "  print(\"Loaded vocab size %i\" % len(vocab))\n",
        "  return wv_from_bin\n",
        "\n",
        "\n",
        "model = load(\"word2vec-google-news-300\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[=================================================-] 99.2% 1649.8/1662.8MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded vocab size 3000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K_XMPY-N4fD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "558dd981-fd42-4e39-e0b5-410905378092"
      },
      "source": [
        "len_word_vec = len(model.get_vector(\"is\"))\n",
        "print(len_word_vec)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Jt9140hTQp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_embedding(word):\n",
        "  if word == \"root\" or word not in model.vocab.keys():\n",
        "        return np.zeros(len_word_vec)\n",
        "  res = model.get_vector(word)\n",
        "  return res\n",
        "\n",
        "def get_word_from_transition(transition, index):\n",
        "  try:\n",
        "    if transition[index] == None:\n",
        "      return None\n",
        "    word = transition[index][word_idx].lower()\n",
        "    return word\n",
        "  except:\n",
        "    print(transition)\n",
        "    print(index)\n",
        "\n",
        "def get_word_pos(words_pos, word):\n",
        "  if word is None or word == \"root\":\n",
        "    return None\n",
        "  return words_pos[word]\n",
        "\n",
        "def get_vector_rep(transitions, words_pos, word_encoding, pos_encoding):  \n",
        "    # one more run on this file to create the vec representation for SVM  \n",
        "    vector_rep = []\n",
        "    \n",
        "    for transition in transitions:        \n",
        "      first_word_Q = get_word_from_transition(transition, 0)\n",
        "      first_word_Q_vec = get_word_embedding(first_word_Q)\n",
        "      first_word__Q_pos_vec = get_one_hot(pos_encoding, get_word_pos(words_pos, first_word_Q))\n",
        "      second_word_Q = get_word_from_transition(transition, 1)\n",
        "      second_word_Q_vec = get_word_embedding(second_word_Q)\n",
        "      second_word_Q_pos_vec = get_one_hot(pos_encoding, get_word_pos(words_pos, second_word_Q))    \n",
        "      first_word_buffer = get_word_from_transition(transition, 2)\n",
        "      first_word_buffer_vec = get_word_embedding(first_word_buffer)\n",
        "      first_word_buffer_pos_vec = get_one_hot(pos_encoding, get_word_pos(words_pos, first_word_buffer))      \n",
        "      transition_rep = np.concatenate((first_word_Q_vec, first_word__Q_pos_vec, second_word_Q_vec, second_word_Q_pos_vec, first_word_buffer_vec, first_word_buffer_pos_vec))\n",
        "      vector_rep.append(transition_rep)      \n",
        "    return np.array(vector_rep)\n",
        "                        \n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDqPt92YdRf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "labels_dict = {'SHIFT': 1, 'LEFT': 2, 'RIGHT': 3}\n",
        "\n",
        "def train(train_file):\n",
        "  sentences, word_encoding, pos_encoding, words_pos = get_sentences(train_file)\n",
        "  print(len(sentences))\n",
        "  labels_vec = []\n",
        "  transitions = []\n",
        "  for sentence in sentences:\n",
        "      res = conll_to_transitions(sentence)\n",
        "      if res is not None:\n",
        "        transitions = transitions + res\n",
        "        labels_vec = labels_vec + [labels_dict[transition[3]] for transition in res]\n",
        "  train_vec = get_vector_rep(transitions, words_pos, word_encoding, pos_encoding)\n",
        "  clf = SVC(gamma='auto')\n",
        "  print(\"we're about to train this classifier!!!\")\n",
        "  clf.fit(train_vec, np.array(labels_vec))\n",
        "  return clf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhKZCIOtRFY1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4ccdcc10-50ac-4448-a8c9-1c597bf1d605"
      },
      "source": [
        "clf = train(train_path)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "982\n",
            "we're about to train this classifier!!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6er52EWexQUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_real_arcs(sentence):\n",
        "  arcs = []\n",
        "  for line in sentence:\n",
        "    arcs.append([line[head_idx], line[word_idx_idx]])\n",
        "  return arcs\n",
        "    \n",
        "    \n",
        "def parser(sentence, clf, pos_encoding, words_pos2):\n",
        "    s = []  #stack\n",
        "    b = deque([])  #buffer\n",
        "    real_arcs = get_real_arcs(sentence)\n",
        "    res_arcs = []\n",
        "    \n",
        "    for w in sentence:\n",
        "        b.append(w)\n",
        "\n",
        "    s.append(['0', 'ROOT', '_', '_', '_', '_', '0', '_', '_', '_'])\n",
        "    transitions = []\n",
        "    \n",
        "    while len(b) > 0 or len(s) > 1:\n",
        "        if (len(b) == 0):\n",
        "          break       \n",
        "        if s[-1][0] == '0':   # the root\n",
        "            add_shift(s, b, transitions)\n",
        "            continue\n",
        "        \n",
        "        first_word_buffer = b[-1]\n",
        "        first_word_q = s[-1]\n",
        "        second_word_q = s[-2]\n",
        "        t = [[first_word_q, second_word_q, first_word_buffer]]\n",
        "        #word_pos = {first_word_q:first_word_q[pos_tag_idx], second_word_q:second_word_q[pos_tag_idx], first_word_buffer:first_word_buffer[pos_tag_idx]}\n",
        "        input_vec = get_vector_rep(t, words_pos2, word_encoding, pos_encoding)\n",
        "        action = clf.predict(input_vec)\n",
        "        \n",
        "        if action == 1: #shift\n",
        "            add_shift(s, b, transitions)\n",
        "        elif action == 2: #left \n",
        "            add_left(s, b, transitions)\n",
        "            res_arcs.append([first_word_q[word_idx_idx], second_word_q[word_idx_idx]])\n",
        "        elif action == 3: #right\n",
        "            add_right(s, b, transitions)\n",
        "            res_arcs.append([second_word_q[word_idx_idx], first_word_q[word_idx_idx]])\n",
        "        else:\n",
        "            print(\"Mew\")\n",
        "          \n",
        "          \n",
        "          \n",
        "    if (len(res_arcs) == 0):\n",
        "      print(\"No arcs\")\n",
        "    #compare arcs\n",
        "    num_true = 0\n",
        "    num_false = 0\n",
        "    for arc in real_arcs:\n",
        "      if arc in res_arcs:\n",
        "        num_true += 1\n",
        "      else:\n",
        "        num_false += 1\n",
        "\n",
        "    return res_arcs, num_true, num_false\n",
        "\n",
        "\n",
        "def evaluate(eval_path, clf, pos_encoding):\n",
        "  sentences, word_encoding2, pos_encoding2, words_pos2 = get_sentences(eval_path)\n",
        "  \n",
        "  num_true = 0\n",
        "  num_false = 0\n",
        "  \n",
        "  for sentence in sentences:\n",
        "        arcs, num_true_s, num_false_s = parser(sentence, clf, pos_encoding, words_pos2)\n",
        "        num_true += num_true_s\n",
        "        num_false += num_false_s\n",
        "\n",
        "  print (f'Accuracy is: {num_true * 100/(num_true + num_false)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRGOj1-WfeQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences, word_encoding, pos_encoding, words_pos = get_sentences(train_path)\n",
        "print(\"starting evaluation\")\n",
        "evaluate(eval_path, clf, pos_encoding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV6qTF4wfhiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}